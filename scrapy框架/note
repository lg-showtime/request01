scrapy 框架
 - 什么是框架？
    - 就是一个集成了很多功能并且具有很强通用性的一个项目框架
 - 如何学习框架
    -专门学习框架封装的各种功能的详细用法
 - 什么是scrapy
    - 爬虫中封装好的一个明星框架
    -环境安装：
        - pip install wheel
        - 下载twisted  地址为：https://www.lfd.uci.edu/~gohlke/pythonlibs/
        - 安装twisted pip install xxxx.whl
        - pip install pywin32
        - pip install scrapy


    - 创建一个工程：scrapy startproject xxxspider
    - cd xxxspider
    - 在scrapy 字目录创建一个爬虫文件
        scrapy genspider spiderName  www.xxx.com
    - 执行工程：
        - scrapy crawl spiderName

- scrapy 持久化存储
    - 基于终端指令
        - 只可以将parse方法的返回值存储到本地的文本文件中
        - 注意：持久化存储对应的文本文件类型中能为：'json','csv','xml'
        - 指定：scrapy crawl xxx -o filepath
        = 好处：简洁高效便捷
        - 缺点：局限性比较强（数据只可以存储到指定后缀的文本文件中）
    - 基于管道
        - 编码流程
            - 数据解析
            - 在item类中定义相关的属性
            - 将解析的数据存储到item类型的对象
            - 将item类型的对象提交给管道进行持久化存储的操作
            - 在管道类中的process_item中要将其接受到的item对象中存储的数据进行持久化存储操作
            - 在配置文件开启管道

- 面试题： 将爬取到的数据一份存储到本地，一份存储到数据库，如何实现？
        - 管道文件中的一个管道类对应的是将数据存储到一个平台
        - 爬虫文件提交的item只会给管道文件中的第一个执行的管道类接受
        - process_item 中的return item 表示将item传递给下一个即将被执行的管道类

- 基于Spider的全站数据的爬取
    - 就是将网站中某板块下的全部对应数据进行爬取
    - 需求：爬取校花网中照片的名称
    - 实现方式：
        - 将所有页码的url添加到start_url当中
        - 自行手动1请求发送（推荐）
- 五大核心组件
引擎(Scrapy)
    用来处理整个系统的数据流处理, 触发事务(框架核心)
调度器(Scheduler)
    用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返回. 可以想像成一个URL（抓取网页的网址或者说是链接）的优先队列, 由它来决定下一个要抓取的网址是什么, 同时去除重复的网址
下载器(Downloader)
    用于下载网页内容, 并将网页内容返回给蜘蛛(Scrapy下载器是建立在twisted这个高效的异步模型上的)
爬虫(Spiders)
    爬虫是主要干活的, 用于从特定的网页中提取自己需要的信息, 即所谓的实体(Item)。用户也可以从中提取出链接,让Scrapy继续抓取下一个页面
项目管道(Pipeline)
    负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息。当页面被爬虫解析后，将被发送到项目管道，并经过几个特定的次序处理数据。

- 请求传参
    - 使用场景：如果爬取的数据不在同一张页面


- 图片保存：
    - ImagePipline:
        - 只需要将image中src属性值进行解析，提交到管道，管道就会对src进行请求发送获取图片得二进制数据
        - 使用流程：
            - 数据解析（图片的地址）
            - 将存储图片地址得item提交到指定的管道类
            - 在管道类文件中自定制一个基于ImagesPipline得一个管道类
                - get_media_request
                - file_path
                - item_completed
            - 在配置文件中：
                -  指定图片存储得目录：IMAGES_STORE ='./imgs_bobo'
                - 指定开启的管到 ：自定制得管道类

- 中间件
    - 下载中间件
        - 位置：引擎和下载器之间
        - 作用：批量拦截到整个工程所有的请求和相应
        - 拦截请求：
            - UA 伪装
            - 代理IP
        - 拦截响应：
            - 篡改响应对象，修该响应数据

- CrawlSpider:类 Spider的一个子类
    - 全站数据爬取的方式
        - 基于Spider：手动请求
        - 基于CrawlSpider
    - CrawlSpider 的使用
        - 创建一个工程
        - cd XXX
        - 创建爬虫文件（CrawlSpider）
            - scrapy genspider -t crawl xxx www.xxx.com
            - 链接提取器：
                   - 作用; 根据指定的规则（allow）进行指定链接的提取
            - 规则解析器：
                   - 作用：将链接提取器提取到链接进行指定规则的解析

- 分布式爬虫
    - 概念：我们需要搭建一个分布式的集群，让其对资源进行分布联合爬取
    - 作用：提升爬取数据的效率

    - 如何实现分布式？
        - 安装一个scrapy-redis 组件
        - 原生的scrapy 是不可以实现分布式爬虫的，必须要让scrapy结合着scrapy-redis 组件一起实现分布式爬虫
        - 为什么原生的scrapy不能实现分布式爬虫？
            - 调度器不可以被分布式集群共享
            - 管道不可以被分布式集群共享
        scrapy-redis 组件作用：
            - 可以给原生的scrapy框架提供可以共享的管道和调度器
            - 实现流程
                - 创建一个工程
                - 创建一个基于Crawlspider的爬虫文件
                - 修改当前的爬虫文件
                    - 导包：from scrapy_redis.spiders import RedisCrawlSpider
                    - 将start_urls和allowed_domains 进行注释
                    - 编写数据解析相关的操作
                    - 将当前爬虫类的父类修改为RedisCrawlSpider
                - 修改配置文件
                    - 指定可以被共享的管道
                    ITEM_PIPELINES ={
                        'scrapy_redis.pipelines.RedisPipeline':400
                    }

                    # 指定调度器
                    # 增加了一个去重容器类的配置，作用使用Redis的set集合来存储请求的指纹数据，从而实现请求去重的持久化
                    DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
                    #使用scrapy_redis 组件自己的调度器
                    SCHEDULER = "scrapy_redis.scheduler.Scheduler"
                    # 配置调度器是否要持久化，也就是当爬虫结束时，要不要清空Redis中请求队列和去重指纹Set，如果True 就是不清空
                    SCHEDULER_PERSIST = True
                  - 指定redis 服务器
                    REDIS_HOST = 'redis服务器的地址'
                    REDIS_PORT = 6379
                    REDIS_ENCODING = 'utf-8'
                - redis 相关操作的配值：
                    - 配置redis 的配置文件：
                        - linux或者mac ： redis.conf
                        - windows : redis.windows.conf
                        - 打开配置文件修改内容：
                            - 将 bind 127.0.0.1 进行注释
                            - 关闭保护模式 protected-mode no
                        - 结合着配置文件开启服务
                            - redis-server 配置文件
                        - 启动客户端：
                            - redis-cli
                    - 执行工程：
                        scrapy runspider xxx.py
                    - 向调度器的队列中放入一个 起始url：
                        - 调度器的队列在redis 的客户端中
                            - lpush xxx www.xxx.com
                    - 爬取到的数据存储在了redis的proName:items 这个数据结构中




